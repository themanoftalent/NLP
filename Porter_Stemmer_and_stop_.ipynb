{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Porter Stemmer and stop .ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/themanoftalent/NLP/blob/main/Porter_Stemmer_and_stop_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BBlXKYHTMj"
      },
      "source": [
        "text= \"\"\"We get the body of text elegantly converted into a list. The above tokenization without NLTK would take hours and hours of coding with regular expressions! You may wonder about the punctuation marks though. This is something we will have to care of separately. We could also use other tokenizers like the PunktSentenceTokenizer, which is a pre-trained unsupervised ML model. We can even train it the ourselves if we want using our own dataset. Keep an eye out for my future articles. insert shameless self-promoting call to follow \"\"\""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKmh4hDiHWW0",
        "outputId": "ae995a7d-7c58-4942-a2e7-f129632ecd85"
      },
      "source": [
        "import nltk \n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SihqL9M-HYkQ"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxNK6gq_H71Q"
      },
      "source": [
        "stop_words=stopwords.words(\"english\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TZphYRwID-s"
      },
      "source": [
        "word_tok = word_tokenize(text)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kIsRLgZIPPZ",
        "outputId": "bc0f87b1-8c81-4c8c-fc1c-8382a434347a"
      },
      "source": [
        "filtered_w = []\n",
        "for word in word_tok:\n",
        "  if word not in stop_words:\n",
        "    filtered_w.append(word)\n",
        "\n",
        "print(text)\n",
        "print(\"\")\n",
        "print(filtered_w)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We get the body of text elegantly converted into a list. The above tokenization without NLTK would take hours and hours of coding with regular expressions! You may wonder about the punctuation marks though. This is something we will have to care of separately. We could also use other tokenizers like the PunktSentenceTokenizer, which is a pre-trained unsupervised ML model. We can even train it the ourselves if we want using our own dataset. Keep an eye out for my future articles. insert shameless self-promoting call to follow \n",
            "\n",
            "['We', 'get', 'body', 'text', 'elegantly', 'converted', 'list', '.', 'The', 'tokenization', 'without', 'NLTK', 'would', 'take', 'hours', 'hours', 'coding', 'regular', 'expressions', '!', 'You', 'may', 'wonder', 'punctuation', 'marks', 'though', '.', 'This', 'something', 'care', 'separately', '.', 'We', 'could', 'also', 'use', 'tokenizers', 'like', 'PunktSentenceTokenizer', ',', 'pre-trained', 'unsupervised', 'ML', 'model', '.', 'We', 'even', 'train', 'want', 'using', 'dataset', '.', 'Keep', 'eye', 'future', 'articles', '.', 'insert', 'shameless', 'self-promoting', 'call', 'follow']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjdsWaTCIeTO"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tinr66hWIgZz",
        "outputId": "21dd62a5-04c8-4b7f-f74f-2dcc71376cea"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps =PorterStemmer()\n",
        "\n",
        "kelime_lisetesi=[\"Swimming\",\"driving\",\"indestrictible\",\"children\",\"geese\",'play', 'playing', 'plays', 'played','playfullness', 'playful']\n",
        "\n",
        "for wordz in kelime_lisetesi:\n",
        "  print(ps.stem(wordz))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "swim\n",
            "drive\n",
            "indestrict\n",
            "children\n",
            "gees\n",
            "play\n",
            "play\n",
            "play\n",
            "play\n",
            "playful\n",
            "play\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K634KxLJJcS4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}